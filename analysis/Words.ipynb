{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part I: Words themes and complexity analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Libs preparation and installation\n",
        "!pip install undetected-chromedriver\n",
        "!pip install pandas numpy nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaKpNiITgsKn"
      },
      "source": [
        "I want to collect mostly used words, find context and level of words from 0 - preschool and 100 - high complexity science literature. Firstly, I'll collect data from these sources:\n",
        "- Literature and books from public domains and libraries for english-speaking countries like UK and USA.\n",
        "- YouTube videos subltitles with highest level of english by themes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibOdIw4CTnBQ"
      },
      "source": [
        "Parsing themes with links from free library [\"Gutenberg\"](http://www.gutenberg.org/ebooks) with `.txt` format for downloading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqbB8RGJUc7k"
      },
      "outputs": [],
      "source": [
        "import undetected_chromedriver as uc\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "driver = uc.Chrome(headless=False)\n",
        "\n",
        "try:\n",
        "    driver.get('https://www.gutenberg.org/ebooks/bookshelf/')\n",
        "    WebDriverWait(driver, 15).until(\n",
        "        EC.presence_of_element_located((By.CLASS_NAME, 'bookshelf_pages'))\n",
        "    )\n",
        "    bookshelves = []\n",
        "    shelf_lists = driver.find_elements(By.CLASS_NAME, 'bookshelf_pages')\n",
        "    for shelf_list in shelf_lists:\n",
        "        items = shelf_list.find_elements(By.TAG_NAME, 'li')\n",
        "        for item in items:\n",
        "            link = item.find_element(By.TAG_NAME, 'a')\n",
        "            text = link.text.strip()\n",
        "            href = link.get_attribute('href')\n",
        "            title = ' '.join(text.split()[1:]) if text.split() and text.split()[0].isdigit() else text\n",
        "            bookshelves.append([title, href])\n",
        "    for i, (title, url) in enumerate(bookshelves, 1):\n",
        "        print(f\"{i:>3}. {title}\\n     {url}\")\n",
        "    print(f\"\\nTotal bookshelves found: {len(bookshelves)}\")\n",
        "    print(bookshelves)\n",
        "except :\n",
        "    # Ensure browser closes even if errors occur\n",
        "    driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7pE7gscTsYg"
      },
      "source": [
        "  1. Best Loved Spanish Literary Classics\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/420\n",
        "  2. Adventure\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/82\n",
        "  3. Africa\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/5\n",
        "  4. African American Writers\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/6\n",
        "  5. Ainslee's\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/195\n",
        "  6. American Revolutionary War\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/196\n",
        "  7. Anarchism\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/7\n",
        "  8. Animal\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/150\n",
        "  9. Animals-Domestic\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/151\n",
        "\n",
        "**...**\n",
        "\n",
        "Total bookshelves found: 338\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VE6ahcbdxWE"
      },
      "source": [
        "After filtering and analysis, new list of most important and common themes looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aPbDXXCfhx6D"
      },
      "outputs": [],
      "source": [
        "processed_list = [\n",
        "    ['Adventure', 'https://www.gutenberg.org/ebooks/bookshelf/82'],\n",
        "    ['Africa', 'https://www.gutenberg.org/ebooks/bookshelf/5'],\n",
        "    ['War', 'https://www.gutenberg.org/ebooks/bookshelf/196'],\n",
        "    ['Anarchism', 'https://www.gutenberg.org/ebooks/bookshelf/7'],\n",
        "    ['Animal', 'https://www.gutenberg.org/ebooks/bookshelf/150'],\n",
        "    ['Domestic', 'https://www.gutenberg.org/ebooks/bookshelf/151'],\n",
        "    ['Birds', 'https://www.gutenberg.org/ebooks/bookshelf/154'],\n",
        "    ['Insects', 'https://www.gutenberg.org/ebooks/bookshelf/155'],\n",
        "    ['Mammals', 'https://www.gutenberg.org/ebooks/bookshelf/156'],\n",
        "    ['Amphibians', 'https://www.gutenberg.org/ebooks/bookshelf/157'],\n",
        "    ['Trapping', 'https://www.gutenberg.org/ebooks/bookshelf/153'],\n",
        "    ['Wild', 'https://www.gutenberg.org/ebooks/bookshelf/152'],\n",
        "    ['Anthropology', 'https://www.gutenberg.org/ebooks/bookshelf/8'],\n",
        "    ['Archaeology', 'https://www.gutenberg.org/ebooks/bookshelf/9'],\n",
        "    ['Architecture', 'https://www.gutenberg.org/ebooks/bookshelf/10'],\n",
        "    ['Argentina', 'https://www.gutenberg.org/ebooks/bookshelf/112'],\n",
        "    ['Art', 'https://www.gutenberg.org/ebooks/bookshelf/11'],\n",
        "    ['Legends', 'https://www.gutenberg.org/ebooks/bookshelf/160'],\n",
        "    ['Astronomy', 'https://www.gutenberg.org/ebooks/bookshelf/101'],\n",
        "    ['Atheism', 'https://www.gutenberg.org/ebooks/bookshelf/199'],\n",
        "    ['Australia', 'https://www.gutenberg.org/ebooks/bookshelf/113'],\n",
        "    ['Racism', 'https://www.gutenberg.org/ebooks/bookshelf/65'],\n",
        "    ['Association', 'https://www.gutenberg.org/ebooks/bookshelf/422'],\n",
        "    ['America', 'https://www.gutenberg.org/ebooks/bookshelf/136'],\n",
        "    ['Listings', 'https://www.gutenberg.org/ebooks/bookshelf/13'],\n",
        "    ['Bibliomania', 'https://www.gutenberg.org/ebooks/bookshelf/15'],\n",
        "    ['Biographies', 'https://www.gutenberg.org/ebooks/bookshelf/16'],\n",
        "    ['Biology', 'https://www.gutenberg.org/ebooks/bookshelf/201'],\n",
        "    ['Botany', 'https://www.gutenberg.org/ebooks/bookshelf/115'],\n",
        "    ['War', 'https://www.gutenberg.org/ebooks/bookshelf/137'],\n",
        "    ['Law', 'https://www.gutenberg.org/ebooks/bookshelf/205'],\n",
        "    ['Buddhism', 'https://www.gutenberg.org/ebooks/bookshelf/116'],\n",
        "    ['Camping', 'https://www.gutenberg.org/ebooks/bookshelf/148'],\n",
        "    ['Canada', 'https://www.gutenberg.org/ebooks/bookshelf/118'],\n",
        "    ['Chemistry', 'https://www.gutenberg.org/ebooks/bookshelf/211'],\n",
        "    ['Series', 'https://www.gutenberg.org/ebooks/bookshelf/17'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/18'],\n",
        "    ['History', 'https://www.gutenberg.org/ebooks/bookshelf/19'],\n",
        "    ['Literature', 'https://www.gutenberg.org/ebooks/bookshelf/20'],\n",
        "    ['Books', 'https://www.gutenberg.org/ebooks/bookshelf/22'],\n",
        "    ['Christianity', 'https://www.gutenberg.org/ebooks/bookshelf/119'],\n",
        "    ['Christmas', 'https://www.gutenberg.org/ebooks/bookshelf/23'],\n",
        "    ['Antiquity', 'https://www.gutenberg.org/ebooks/bookshelf/24'],\n",
        "    ['Cooking', 'https://www.gutenberg.org/ebooks/bookshelf/419'],\n",
        "    ['Crafts', 'https://www.gutenberg.org/ebooks/bookshelf/27'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/28'],\n",
        "    ['Nonfiction', 'https://www.gutenberg.org/ebooks/bookshelf/29'],\n",
        "    ['History', 'https://www.gutenberg.org/ebooks/bookshelf/220'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/30'],\n",
        "    ['Society', 'https://www.gutenberg.org/ebooks/bookshelf/31'],\n",
        "    ['Engineering', 'https://www.gutenberg.org/ebooks/bookshelf/32'],\n",
        "    ['War', 'https://www.gutenberg.org/ebooks/bookshelf/139'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/33'],\n",
        "    ['Esperanto', 'https://www.gutenberg.org/ebooks/bookshelf/34'],\n",
        "    ['Ecology', 'https://www.gutenberg.org/ebooks/bookshelf/224'],\n",
        "    ['Education', 'https://www.gutenberg.org/ebooks/bookshelf/138'],\n",
        "    ['Egypt', 'https://www.gutenberg.org/ebooks/bookshelf/121'],\n",
        "    ['Fantasy', 'https://www.gutenberg.org/ebooks/bookshelf/36'],\n",
        "    ['Folklore', 'https://www.gutenberg.org/ebooks/bookshelf/37'],\n",
        "    ['Forestry', 'https://www.gutenberg.org/ebooks/bookshelf/145'],\n",
        "    ['France', 'https://www.gutenberg.org/ebooks/bookshelf/122'],\n",
        "    ['Forest', 'https://www.gutenberg.org/ebooks/bookshelf/226'],\n",
        "    ['Geology', 'https://www.gutenberg.org/ebooks/bookshelf/227'],\n",
        "    ['Germany', 'https://www.gutenberg.org/ebooks/bookshelf/123'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/39'],\n",
        "    ['Greece', 'https://www.gutenberg.org/ebooks/bookshelf/124'],\n",
        "    ['Classics', 'https://www.gutenberg.org/ebooks/bookshelf/40'],\n",
        "    ['Hinduism', 'https://www.gutenberg.org/ebooks/bookshelf/125'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/41'],\n",
        "    ['Horror', 'https://www.gutenberg.org/ebooks/bookshelf/42'],\n",
        "    ['Horticulture', 'https://www.gutenberg.org/ebooks/bookshelf/43'],\n",
        "    ['Humor', 'https://www.gutenberg.org/ebooks/bookshelf/44'],\n",
        "    ['India', 'https://www.gutenberg.org/ebooks/bookshelf/45'],\n",
        "    ['Islam', 'https://www.gutenberg.org/ebooks/bookshelf/126'],\n",
        "    ['Italy', 'https://www.gutenberg.org/ebooks/bookshelf/127'],\n",
        "    ['Judaism', 'https://www.gutenberg.org/ebooks/bookshelf/128'],\n",
        "    ['Education', 'https://www.gutenberg.org/ebooks/bookshelf/46'],\n",
        "    ['Love', 'https://www.gutenberg.org/ebooks/bookshelf/47'],\n",
        "    ['Manufacturing', 'https://www.gutenberg.org/ebooks/bookshelf/146'],\n",
        "    ['Mathematics', 'https://www.gutenberg.org/ebooks/bookshelf/102'],\n",
        "    ['Medicine', 'https://www.gutenberg.org/ebooks/bookshelf/48'],\n",
        "    ['Microbiology', 'https://www.gutenberg.org/ebooks/bookshelf/105'],\n",
        "    ['Microscopy', 'https://www.gutenberg.org/ebooks/bookshelf/109'],\n",
        "    ['Books', 'https://www.gutenberg.org/ebooks/bookshelf/49'],\n",
        "    ['Music', 'https://www.gutenberg.org/ebooks/bookshelf/50'],\n",
        "    ['Mycology', 'https://www.gutenberg.org/ebooks/bookshelf/129'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/51'],\n",
        "    ['Mythology', 'https://www.gutenberg.org/ebooks/bookshelf/52'],\n",
        "    ['Bookshelf', 'https://www.gutenberg.org/ebooks/bookshelf/149'],\n",
        "    ['America', 'https://www.gutenberg.org/ebooks/bookshelf/53'],\n",
        "    ['History', 'https://www.gutenberg.org/ebooks/bookshelf/54'],\n",
        "    ['Zealand', 'https://www.gutenberg.org/ebooks/bookshelf/130'],\n",
        "    ['Association', 'https://www.gutenberg.org/ebooks/bookshelf/244'],\n",
        "    ['Norway', 'https://www.gutenberg.org/ebooks/bookshelf/131'],\n",
        "    ['Plays', 'https://www.gutenberg.org/ebooks/bookshelf/55'],\n",
        "    ['Opera', 'https://www.gutenberg.org/ebooks/bookshelf/56'],\n",
        "    ['Paganism', 'https://www.gutenberg.org/ebooks/bookshelf/132'],\n",
        "    ['Philosophy', 'https://www.gutenberg.org/ebooks/bookshelf/57'],\n",
        "    ['Photography', 'https://www.gutenberg.org/ebooks/bookshelf/158'],\n",
        "    ['Physics', 'https://www.gutenberg.org/ebooks/bookshelf/103'],\n",
        "    ['Physiology', 'https://www.gutenberg.org/ebooks/bookshelf/110'],\n",
        "    ['Plays', 'https://www.gutenberg.org/ebooks/bookshelf/59'],\n",
        "    ['Poetry', 'https://www.gutenberg.org/ebooks/bookshelf/60'],\n",
        "    ['Politics', 'https://www.gutenberg.org/ebooks/bookshelf/61'],\n",
        "    ['Precursors', 'https://www.gutenberg.org/ebooks/bookshelf/62'],\n",
        "    ['Gutenberg', 'https://www.gutenberg.org/ebooks/bookshelf/63'],\n",
        "    ['Psychology', 'https://www.gutenberg.org/ebooks/bookshelf/64'],\n",
        "    ['Reference', 'https://www.gutenberg.org/ebooks/bookshelf/66'],\n",
        "    ['Stories', 'https://www.gutenberg.org/ebooks/bookshelf/67'],\n",
        "    ['Science', 'https://www.gutenberg.org/ebooks/bookshelf/106'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/68'],\n",
        "    ['Women', 'https://www.gutenberg.org/ebooks/bookshelf/403'],\n",
        "    ['Scouts', 'https://www.gutenberg.org/ebooks/bookshelf/144'],\n",
        "    ['Stories', 'https://www.gutenberg.org/ebooks/bookshelf/69'],\n",
        "    ['Slavery', 'https://www.gutenberg.org/ebooks/bookshelf/70'],\n",
        "    ['Sociology', 'https://www.gutenberg.org/ebooks/bookshelf/134'],\n",
        "    ['Africa', 'https://www.gutenberg.org/ebooks/bookshelf/135'],\n",
        "    ['America', 'https://www.gutenberg.org/ebooks/bookshelf/71'],\n",
        "    ['War', 'https://www.gutenberg.org/ebooks/bookshelf/140'],\n",
        "    ['Suffrage', 'https://www.gutenberg.org/ebooks/bookshelf/72'],\n",
        "    ['Technology', 'https://www.gutenberg.org/ebooks/bookshelf/143'],\n",
        "    ['Microbiology', 'https://www.gutenberg.org/ebooks/bookshelf/105'],\n",
        "    ['Journal', 'https://www.gutenberg.org/ebooks/bookshelf/423'],\n",
        "    ['Transportation', 'https://www.gutenberg.org/ebooks/bookshelf/74'],\n",
        "    ['Travel', 'https://www.gutenberg.org/ebooks/bookshelf/75'],\n",
        "    ['War', 'https://www.gutenberg.org/ebooks/bookshelf/141'],\n",
        "    ['Kingdom', 'https://www.gutenberg.org/ebooks/bookshelf/76'],\n",
        "    ['States', 'https://www.gutenberg.org/ebooks/bookshelf/136'],\n",
        "    ['Law', 'https://www.gutenberg.org/ebooks/bookshelf/302'],\n",
        "    ['Western', 'https://www.gutenberg.org/ebooks/bookshelf/77'],\n",
        "    ['Witchcraft', 'https://www.gutenberg.org/ebooks/bookshelf/78'],\n",
        "    ['Journals', 'https://www.gutenberg.org/ebooks/bookshelf/80'],\n",
        "    ['Woodwork', 'https://www.gutenberg.org/ebooks/bookshelf/147'],\n",
        "    ['War', 'https://www.gutenberg.org/ebooks/bookshelf/142'],\n",
        "    ['War', 'https://www.gutenberg.org/ebooks/bookshelf/325'],\n",
        "    ['Zoology', 'https://www.gutenberg.org/ebooks/bookshelf/303']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEvTS668pw8M"
      },
      "source": [
        "Parsing several (25 maximum) texts per theme of books for these themes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPjqTG0xp9_B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import undetected_chromedriver as uc\n",
        "\n",
        "# Initialize the driver\n",
        "driver = uc.Chrome(headless=True)\n",
        "\n",
        "def download_file(url, save_path):\n",
        "    \"\"\"Download a file from URL to specified path\"\"\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def process_book(book_url, theme_dir):\n",
        "    \"\"\"Process individual book page and download if conditions are met\"\"\"\n",
        "    try:\n",
        "        driver.get(book_url)\n",
        "        print(f\"{theme_dir}:{book_url}\")\n",
        "\n",
        "        # 1. Check language (wait for element to load)\n",
        "        try:\n",
        "            lang_xpath = \"//table[@class='bibrec']//tr[th='Language']/td\"\n",
        "            lang_element = WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_element_located((By.XPATH, lang_xpath))\n",
        "            )\n",
        "            language = lang_element.text.strip()\n",
        "            print(f\"Language {language}\")\n",
        "            if language not in [\"English\", \"Английский\"]:\n",
        "                return False\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "        # 2. Extract reading ease score\n",
        "        try:\n",
        "            # Use presence_of_all_elements_located to find every element that matches\n",
        "            note_xpath = \"//table[@class='bibrec']//tr[th='Note']/td\"\n",
        "            note_elements = WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_all_elements_located((By.XPATH, note_xpath))\n",
        "            )\n",
        "\n",
        "            # Check if any notes were found\n",
        "            if not note_elements:\n",
        "                return False\n",
        "\n",
        "            all_notes_data = []\n",
        "            reading_score = None\n",
        "\n",
        "            # Loop through each found note element\n",
        "            for note_element in note_elements:\n",
        "                note_text = note_element.text.strip()\n",
        "                print(f\"Note: {note_text}\")\n",
        "\n",
        "                # Extract reading score from each note using regex\n",
        "                match = re.search(r\"Reading ease score:\\s*([\\d.]+)\", note_text)\n",
        "\n",
        "                current_score = match.group(1) if match else None\n",
        "\n",
        "                # Store the first valid reading score found\n",
        "                if current_score and reading_score is None:\n",
        "                    reading_score = current_score\n",
        "\n",
        "                all_notes_data.append({\n",
        "                    \"note_text\": note_text,\n",
        "                    \"reading_score\": current_score\n",
        "                })\n",
        "\n",
        "            # Check if at least one note had a reading score.\n",
        "            if reading_score is None:\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while parsing notes: {e}\")\n",
        "            return False\n",
        "\n",
        "        # 3. Find and process download link\n",
        "        try:\n",
        "            selectors = [\n",
        "                \"//a[@class='link ' and text()='Plain Text UTF-8']\"\n",
        "            ]\n",
        "\n",
        "            download_link = None\n",
        "            for selector in selectors:\n",
        "                try:\n",
        "                    # Use find_elements to avoid an exception if not found\n",
        "                    links = driver.find_elements(By.XPATH, selector)\n",
        "                    if links:\n",
        "                        download_link = links[0]\n",
        "                        break\n",
        "                except Exception:\n",
        "                    # Continue to the next selector\n",
        "                    continue\n",
        "\n",
        "            if not download_link:\n",
        "                print(\"Error: Download link not found.\")\n",
        "                return False\n",
        "\n",
        "            download_url = download_link.get_attribute('href')\n",
        "            book_id_match = re.search(r'/ebooks/(\\d+)', book_url)\n",
        "            if not book_id_match:\n",
        "                print(\"Error: Could not extract book ID from URL.\")\n",
        "                return False\n",
        "\n",
        "            book_id = book_id_match.group(1)\n",
        "            filename = f\"{book_id}_{reading_score}.txt\"\n",
        "            save_path = os.path.join(theme_dir, filename)\n",
        "\n",
        "            # Download the file\n",
        "            if download_file(download_url, save_path):\n",
        "                print(f\"Downloaded: {filename}\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"Failed to download: {filename}\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during download link processing: {e}\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {book_url}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "    return False\n",
        "\n",
        "# Main processing loop\n",
        "for theme_name, theme_url in processed_list:\n",
        "    try:\n",
        "        # Create theme directory\n",
        "        theme_dir = re.sub(r'[\\\\/*?:\"<>|]', \"\", theme_name)  # Sanitize directory name\n",
        "        os.makedirs('data/' + theme_dir, exist_ok=True)\n",
        "        print(f\"\\nProcessing theme: {theme_name}\")\n",
        "\n",
        "        # Navigate to bookshelf page\n",
        "        driver.get(theme_url)\n",
        "        time.sleep(2)  # Initial load wait\n",
        "\n",
        "        # Process pagination\n",
        "        page_count = 0\n",
        "        books_processed = 0\n",
        "        while True:\n",
        "            page_count += 1\n",
        "            print(f\"  Page {page_count}\")\n",
        "\n",
        "            # Find all book links\n",
        "            book_links = []\n",
        "            try:\n",
        "                book_elements = WebDriverWait(driver, 15).until(\n",
        "                    EC.presence_of_all_elements_located((By.XPATH, \"//li[@class='booklink']/a[@class='link']\"))\n",
        "                )\n",
        "                book_links = [elem.get_attribute('href') for elem in book_elements]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Process each book\n",
        "            for book_url in book_links:\n",
        "                if process_book(book_url, theme_dir):\n",
        "                    books_processed += 1\n",
        "                time.sleep(1)  # Be polite to server\n",
        "\n",
        "            # Check for next page\n",
        "            try:\n",
        "                next_button = driver.find_element(By.XPATH, \"//a[@title='Go to next page']\")\n",
        "                if \"disabled\" in next_button.get_attribute(\"class\"):\n",
        "                    break\n",
        "\n",
        "                next_button.click()\n",
        "                time.sleep(3)  # Wait for page load\n",
        "            except:\n",
        "                break\n",
        "\n",
        "        print(f\"  Finished theme: {theme_name} | Books downloaded: {books_processed}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing theme {theme_name}: {str(e)}\")\n",
        "\n",
        "# Clean up\n",
        "driver.quit()\n",
        "print(\"\\nProcessing completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRDi8IjqwhP1"
      },
      "source": [
        "After 3 hours of executing, we get 1.4 Gb of text data with theme and text reading complexity separation. File saved in directories by theme, file name saved as `{BookId}_{TextComplexity}.txt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL8U2cd-4Bm9",
        "outputId": "2c2c3d9b-0e05-44a2-9625-885bb47f9911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adventure, Africa, America, Amphibians, Anarchism, Animal, Anthropology, Antiquity, Archaeology, Architecture, Argentina, Art, Association, Astronomy, Atheism, Australia, Bibliomania, Biographies, Biology, Birds, Books, Bookshelf, Botany, Buddhism, Camping, Canada, Chemistry, Christianity, Christmas, Classics, Cooking, Crafts, Domestic, Ecology, Education, Egypt, Engineering, Esperanto, Fantasy, Fiction, Folklore, Forest, Forestry, France, Geology, Germany, Greece, Gutenberg, Hinduism, History, Horror, Horticulture, Humor, India, Insects, Islam, Italy, Journal, Journals, Judaism, Kingdom, Law, Legends, Listings, Literature, Love, Mammals, Manufacturing, Mathematics, Medicine, Microbiology, Microscopy, Music, Mycology, Mythology, Nonfiction, Norway, Opera, Paganism, Philosophy, Photography, Physics, Physiology, Plays, Poetry, Politics, Precursors, Psychology, Racism, Reference, Science, Scouts, Series, Slavery, Society, Sociology, States, Stories, Suffrage, Technology, Transportation, Trapping, Travel, War, Western, Wild, Witchcraft, Women, Woodwork, Zealand, Zoology\n"
          ]
        }
      ],
      "source": [
        "dirs = [element[0] for element in processed_list]\n",
        "dirs = list(set(dirs))\n",
        "dirs.sort()\n",
        "print(\", \".join(dirs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg4eR5046bFk"
      },
      "source": [
        "Now creating \"one word\" dataset with these columns:\n",
        "\n",
        "| Word | Adventure | Africa | ... | Zealand | Zoology | Total | Complexity |\n",
        "|-|-|-|-|-|-|-|-|\n",
        "| `str` | `int` | `int` | ... | `int` | `int` | `int` | `float` |\n",
        "\n",
        "by this algorithm:\n",
        "1. Open text from every directory (theme)\n",
        "2. Clear and tokenize text by NLTK, deleting stop words  \n",
        "2. Adding every token:\n",
        "  - if exist, update theme counter, total counter and complexity by this formula:\n",
        "  $\n",
        "  \\text{complexity}_{\\text{new}} = \\frac{\\text{comlexity}_\\text{old}\\cdot \\text{total}+\\text{text_complexity}}{\\text{total}+1}\n",
        "  $\n",
        "  - if not exit, add word update counters and set complxity to text complexity of current text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFEVgONg92C2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\79150\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\79150\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\79150\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\79150\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 111 themes: Adventure, Africa, America, Amphibians, Anarchism, Animal, Anthropology, Antiquity, Archaeology, Architecture, Argentina, Art, Association, Astronomy, Atheism, Australia, Bibliomania, Biographies, Biology, Birds, Books, Bookshelf, Botany, Buddhism, Camping, Canada, Chemistry, Christianity, Christmas, Classics, Cooking, Crafts, Domestic, Ecology, Education, Egypt, Engineering, Esperanto, Fantasy, Fiction, Folklore, Forest, Forestry, France, Geology, Germany, Greece, Gutenberg, Hinduism, History, Horror, Horticulture, Humor, India, Insects, Islam, Italy, Journal, Journals, Judaism, Kingdom, Law, Legends, Listings, Literature, Love, Mammals, Manufacturing, Mathematics, Medicine, Microbiology, Microscopy, Music, Mycology, Mythology, Nonfiction, Norway, Opera, Paganism, Philosophy, Photography, Physics, Physiology, Plays, Poetry, Politics, Precursors, Psychology, Racism, Reference, Science, Scouts, Series, Slavery, Society, Sociology, States, Stories, Suffrage, Technology, Transportation, Trapping, Travel, War, Western, Wild, Witchcraft, Women, Woodwork, Zealand, Zoology\n",
            "Processing theme: Adventure\n",
            "Processing theme: Africa\n",
            "Processing theme: America\n",
            "Processing theme: Amphibians\n",
            "Processing theme: Anarchism\n",
            "Processing theme: Animal\n",
            "Processing theme: Anthropology\n",
            "Processing theme: Antiquity\n",
            "Processing theme: Archaeology\n",
            "Processing theme: Architecture\n",
            "Processing theme: Argentina\n",
            "Processing theme: Art\n",
            "Processing theme: Association\n",
            "Processing theme: Astronomy\n",
            "Processing theme: Atheism\n",
            "Processing theme: Australia\n",
            "Processing theme: Bibliomania\n",
            "Processing theme: Biographies\n",
            "Processing theme: Biology\n",
            "Processing theme: Birds\n",
            "Processing theme: Books\n",
            "Processing theme: Bookshelf\n",
            "Processing theme: Botany\n",
            "Processing theme: Buddhism\n",
            "Processing theme: Camping\n",
            "Processing theme: Canada\n",
            "Processing theme: Chemistry\n",
            "Processing theme: Christianity\n",
            "Processing theme: Christmas\n",
            "Processing theme: Classics\n",
            "Processing theme: Cooking\n",
            "Processing theme: Crafts\n",
            "Processing theme: Domestic\n",
            "Processing theme: Ecology\n",
            "Processing theme: Education\n",
            "Processing theme: Egypt\n",
            "Processing theme: Engineering\n",
            "Processing theme: Esperanto\n",
            "Processing theme: Fantasy\n",
            "Processing theme: Fiction\n",
            "Processing theme: Folklore\n",
            "Processing theme: Forest\n",
            "Processing theme: Forestry\n",
            "Processing theme: France\n",
            "Processing theme: Geology\n",
            "Processing theme: Germany\n",
            "Processing theme: Greece\n",
            "Processing theme: Gutenberg\n",
            "Processing theme: Hinduism\n",
            "Processing theme: History\n",
            "Processing theme: Horror\n",
            "Processing theme: Horticulture\n",
            "Processing theme: Humor\n",
            "Processing theme: India\n",
            "Processing theme: Insects\n",
            "Processing theme: Islam\n",
            "Processing theme: Italy\n",
            "Processing theme: Journal\n",
            "Processing theme: Journals\n",
            "Processing theme: Judaism\n",
            "Processing theme: Kingdom\n",
            "Processing theme: Law\n",
            "Processing theme: Legends\n",
            "Processing theme: Listings\n",
            "Processing theme: Literature\n",
            "Processing theme: Love\n",
            "Processing theme: Mammals\n",
            "Processing theme: Manufacturing\n",
            "Processing theme: Mathematics\n",
            "Processing theme: Medicine\n",
            "Processing theme: Microbiology\n",
            "Processing theme: Microscopy\n",
            "Processing theme: Music\n",
            "Processing theme: Mycology\n",
            "Processing theme: Mythology\n",
            "Processing theme: Nonfiction\n",
            "Processing theme: Norway\n",
            "Processing theme: Opera\n",
            "Processing theme: Paganism\n",
            "Processing theme: Philosophy\n",
            "Processing theme: Photography\n",
            "Processing theme: Physics\n",
            "Processing theme: Physiology\n",
            "Processing theme: Plays\n",
            "Processing theme: Poetry\n",
            "Processing theme: Politics\n",
            "Processing theme: Precursors\n",
            "Processing theme: Psychology\n",
            "Processing theme: Racism\n",
            "Processing theme: Reference\n",
            "Processing theme: Science\n",
            "Processing theme: Scouts\n",
            "Processing theme: Series\n",
            "Processing theme: Slavery\n",
            "Processing theme: Society\n",
            "Processing theme: Sociology\n",
            "Processing theme: States\n",
            "Processing theme: Stories\n",
            "Processing theme: Suffrage\n",
            "Processing theme: Technology\n",
            "Processing theme: Transportation\n",
            "Processing theme: Trapping\n",
            "Processing theme: Travel\n",
            "Processing theme: War\n",
            "Processing theme: Western\n",
            "Processing theme: Wild\n",
            "Processing theme: Witchcraft\n",
            "Processing theme: Women\n",
            "Processing theme: Woodwork\n",
            "Processing theme: Zealand\n",
            "Processing theme: Zoology\n",
            "\n",
            "Dataset created with 1351701 unique words\n",
            "Top 10 most frequent words:\n",
            "       Word   Total  Complexity\n",
            "52      one  749024   67.143501\n",
            "608   would  477428   67.192777\n",
            "140    said  460270   74.127681\n",
            "683    time  406210   66.731788\n",
            "17      may  384135   64.051919\n",
            "1232   work  351692   66.587452\n",
            "54      man  326321   68.929421\n",
            "945    upon  322783   65.671136\n",
            "161     two  321086   66.728969\n",
            "6       day  301614   68.404165\n",
            "\n",
            "Dataset saved to word_dataset.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Tokenize, clean, and lemmatize text\"\"\"\n",
        "    # Convert to lowercase and remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and short tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "def process_directory(base_dir, dirs):\n",
        "    \"\"\"Process all theme directories and create the word dataset\"\"\"\n",
        "    # Filter themes to only include those in dirs list\n",
        "    themes = [d for d in dirs if os.path.isdir(os.path.join(base_dir, d))]\n",
        "    print(f\"Processing {len(themes)} themes: {', '.join(themes)}\")\n",
        "\n",
        "    # Initialize word dictionary\n",
        "    word_data = {}\n",
        "\n",
        "    # Process each theme\n",
        "    for theme in themes:\n",
        "        theme_dir = os.path.join(base_dir, theme)\n",
        "        print(f\"Processing theme: {theme}\")\n",
        "\n",
        "        # Process each file in the theme directory\n",
        "        for filename in os.listdir(theme_dir):\n",
        "            if filename.endswith('.txt'):\n",
        "                # Extract reading score from filename\n",
        "                try:\n",
        "                    book_id, reading_score = filename.split('_')[:2]\n",
        "                    reading_score = float(reading_score.replace('.txt', ''))\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing filename {filename}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                filepath = os.path.join(theme_dir, filename)\n",
        "\n",
        "                try:\n",
        "                    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        text = f.read()\n",
        "\n",
        "                    # Preprocess text\n",
        "                    tokens = preprocess_text(text)\n",
        "\n",
        "                    # Update word data\n",
        "                    for word in tokens:\n",
        "                        if word not in word_data:\n",
        "                            # Initialize new word entry\n",
        "                            word_data[word] = {\n",
        "                                'Total': 0,\n",
        "                                'Complexity': 0.0,\n",
        "                                **{t: 0 for t in themes}  # Initialize all themes to 0\n",
        "                            }\n",
        "\n",
        "                        # Get current word stats\n",
        "                        current = word_data[word]\n",
        "                        total_old = current['Total']\n",
        "\n",
        "                        # Update counts\n",
        "                        current['Total'] += 1\n",
        "                        current[theme] += 1\n",
        "\n",
        "                        # Update complexity using moving average formula\n",
        "                        current['Complexity'] = (\n",
        "                            (current['Complexity'] * total_old) + reading_score\n",
        "                        ) / (total_old + 1)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame.from_dict(word_data, orient='index')\n",
        "    df.reset_index(inplace=True)\n",
        "    df.rename(columns={'index': 'Word'}, inplace=True)\n",
        "\n",
        "    # Reorder columns: Word, themes, Total, Complexity\n",
        "    columns = ['Word'] + themes + ['Total', 'Complexity']\n",
        "    return df[columns]\n",
        "\n",
        "# Main processing\n",
        "base_directory = \"Data\"  # Directory containing theme subdirectories\n",
        "output_file = \"word_dataset.csv\"\n",
        "\n",
        "# Use the dirs array from the previous cell\n",
        "df = process_directory(base_directory, dirs)\n",
        "\n",
        "# Display some statistics\n",
        "print(f\"\\nDataset created with {len(df)} unique words\")\n",
        "print(f\"Top 10 most frequent words:\")\n",
        "print(df.sort_values('Total', ascending=False).head(10)[['Word', 'Total', 'Complexity']])\n",
        "\n",
        "# Save results\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"\\nDataset saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Total</th>\n",
              "      <th>Complexity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>one</td>\n",
              "      <td>749024</td>\n",
              "      <td>67.143501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>608</th>\n",
              "      <td>would</td>\n",
              "      <td>477428</td>\n",
              "      <td>67.192777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>said</td>\n",
              "      <td>460270</td>\n",
              "      <td>74.127681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>683</th>\n",
              "      <td>time</td>\n",
              "      <td>406210</td>\n",
              "      <td>66.731788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>may</td>\n",
              "      <td>384135</td>\n",
              "      <td>64.051919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1437</th>\n",
              "      <td>set</td>\n",
              "      <td>113125</td>\n",
              "      <td>67.948997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>order</td>\n",
              "      <td>111802</td>\n",
              "      <td>63.708995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>869</th>\n",
              "      <td>far</td>\n",
              "      <td>111664</td>\n",
              "      <td>65.489456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>world</td>\n",
              "      <td>111031</td>\n",
              "      <td>65.826793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>whole</td>\n",
              "      <td>110181</td>\n",
              "      <td>64.430272</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Word   Total  Complexity\n",
              "52      one  749024   67.143501\n",
              "608   would  477428   67.192777\n",
              "140    said  460270   74.127681\n",
              "683    time  406210   66.731788\n",
              "17      may  384135   64.051919\n",
              "...     ...     ...         ...\n",
              "1437    set  113125   67.948997\n",
              "229   order  111802   63.708995\n",
              "869     far  111664   65.489456\n",
              "4     world  111031   65.826793\n",
              "122   whole  110181   64.430272\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import df from csv\n",
        "df = pd.read_csv('word_dataset.csv')\n",
        "# Drop all words with total less than 10\n",
        "df = df[df['Total'] > 10]\n",
        "\n",
        "df.sort_values('Total', ascending=False).head(100)[['Word', 'Total', 'Complexity']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF0aAy5cG57x"
      },
      "source": [
        "After computing comlexity of words, we also can add oral English style data by parsing YouTube videos per theme and uploading thier subtitles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Exercises\n",
        "Not only vocabulary is important, but also the way words are used. To practice the user's use of new words as well as learning grammar, syntax, and semantics, we will create a database of exercises using textbooks, websites, other resources, and create a model based on textual data to predict difficult moments in sentenses and generate assignments with solutions for them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Anton Korotkov, ML ingineer. </br>\n",
        "I have recieved data from George and analyzed which approaches might be useful for our purposes. Basically, for our goal we need to design an architecture that would analyze the dataset of words and educational datasets(for example, open bases of English tasks) and based on some word generate a task. To build this architecture we need to explicitely define the problem:</br>\n",
        "**Text-to-Task Generation:**\n",
        "- Input: (word, complexity, context, topic)\n",
        "- Output: A task (e.g., fill-in-the-blank, match with definition, generate a sentence, etc.)</br>\n",
        "\n",
        "There are 3 approaches to reach our goal:\n",
        "\n",
        "- **Encoder-Decoder Models(seq2seq)**:\n",
        "    - T5(Text-to-text transfer Transformer):\n",
        "        - Every task is a text generation task\n",
        "        - Input: \"Generate vocabulary exercise for: [word], complexity: B1, context: ..., topic: Health\"\n",
        "        - Output: \"Fill in the blank: You should always wear a ____ when cycling. (helmet)\"\n",
        "        - In is very useful to follow this approach, because it is flexible and pretrained on NLP tasks and it is **fine-tunable**\n",
        "    - BART (Bidirectional and Auto-Regressive Transformers)\n",
        "        - Similar to T5, but trained for denoising tasks and generation\n",
        "        - More suitable for masked-style tasks (e.g., cloze tasks)\n",
        "    - FLAN-T5 / mT5\n",
        "        - Fine-tuned versions of T5 with better zero-shot generalization\n",
        "\n",
        "- **Prompt-based Models (Instruction-Tuned LLMs)**\n",
        "    - GPT-4 or GPT-3.5 (via OpenAI API)\n",
        "    - LLaMA 3 / Mistral / Mixtral\n",
        "    - Claude (Anthropic)\n",
        "    - These models can be prompted like:</br>\n",
        "        - *Given the word \"photosynthesis\", complexity level \"B2\", context \"The process plants use to make food...\", and topic \"Biology\", generate a vocabulary exercise appropriate for a B2-level English learner.*</br>\n",
        "    - We can use this architecture for starting the whole system since using this structure is very **lightweight** and does not need heavy fine-tuning.\n",
        "- **Fine-tuned Task Generators**\n",
        "    - This schema is robust to make a pedagogically focused outputs, since it is made with emphasis on using with academical purposes\n",
        "    - RUBERT or RoBERTa fine-tuned on educational datasets\n",
        "    - Task-specific fine-tuning on datasets like:\n",
        "        - George's work(open Literature datasets)\n",
        "        - BEA(Building Education Applications)\n",
        "        - Newsela (simplified texts by grade level)\n",
        "        - TNT (Teacher Needs Tasks) datasets.\n",
        "During the next weeks we will decide which approach we will use for our architecture and develop it rapidly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
