{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Words themes and complexity analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Libs preparation and installation\n",
        "!pip install undetected-chromedriver\n",
        "!pip install pandas numpy nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaKpNiITgsKn"
      },
      "source": [
        "I want to collect mostly used words, find context and level of words from 0 - preschool and 100 - high complexity science literature. Firstly, I'll collect data from these sources:\n",
        "- Literature and books from public domains and libraries for english-speaking countries like UK and USA.\n",
        "- YouTube videos subltitles with highest level of english by themes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibOdIw4CTnBQ"
      },
      "source": [
        "Parsing themes with links from free library [\"Gutenberg\"](http://www.gutenberg.org/ebooks) with `.txt` format for downloading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqbB8RGJUc7k"
      },
      "outputs": [],
      "source": [
        "import undetected_chromedriver as uc\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "driver = uc.Chrome(headless=False)\n",
        "\n",
        "try:\n",
        "    driver.get('https://www.gutenberg.org/ebooks/bookshelf/')\n",
        "    WebDriverWait(driver, 15).until(\n",
        "        EC.presence_of_element_located((By.CLASS_NAME, 'bookshelf_pages'))\n",
        "    )\n",
        "    bookshelves = []\n",
        "    shelf_lists = driver.find_elements(By.CLASS_NAME, 'bookshelf_pages')\n",
        "    for shelf_list in shelf_lists:\n",
        "        items = shelf_list.find_elements(By.TAG_NAME, 'li')\n",
        "        for item in items:\n",
        "            link = item.find_element(By.TAG_NAME, 'a')\n",
        "            text = link.text.strip()\n",
        "            href = link.get_attribute('href')\n",
        "            title = ' '.join(text.split()[1:]) if text.split() and text.split()[0].isdigit() else text\n",
        "            bookshelves.append([title, href])\n",
        "    for i, (title, url) in enumerate(bookshelves, 1):\n",
        "        print(f\"{i:>3}. {title}\\n     {url}\")\n",
        "    print(f\"\\nTotal bookshelves found: {len(bookshelves)}\")\n",
        "    print(bookshelves)\n",
        "except :\n",
        "    # Ensure browser closes even if errors occur\n",
        "    driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7pE7gscTsYg"
      },
      "source": [
        "  1. Best Loved Spanish Literary Classics\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/420\n",
        "  2. Adventure\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/82\n",
        "  3. Africa\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/5\n",
        "  4. African American Writers\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/6\n",
        "  5. Ainslee's\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/195\n",
        "  6. American Revolutionary War\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/196\n",
        "  7. Anarchism\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/7\n",
        "  8. Animal\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/150\n",
        "  9. Animals-Domestic\n",
        "     https://www.gutenberg.org/ebooks/bookshelf/151\n",
        "\n",
        "**...**\n",
        "\n",
        "Total bookshelves found: 338\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VE6ahcbdxWE"
      },
      "source": [
        "After filtering and analysis, new list of most important and common themes looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aPbDXXCfhx6D"
      },
      "outputs": [],
      "source": [
        "processed_list = [\n",
        "    ['Adventure', 'https://www.gutenberg.org/ebooks/bookshelf/82'],\n",
        "    ['Africa', 'https://www.gutenberg.org/ebooks/bookshelf/5'],\n",
        "    ['War', 'https://www.gutenberg.org/ebooks/bookshelf/196'],\n",
        "    ['Anarchism', 'https://www.gutenberg.org/ebooks/bookshelf/7'],\n",
        "    ['Animal', 'https://www.gutenberg.org/ebooks/bookshelf/150'],\n",
        "    ['Domestic', 'https://www.gutenberg.org/ebooks/bookshelf/151'],\n",
        "    ['Birds', 'https://www.gutenberg.org/ebooks/bookshelf/154'],\n",
        "    ['Insects', 'https://www.gutenberg.org/ebooks/bookshelf/155'],\n",
        "    ['Mammals', 'https://www.gutenberg.org/ebooks/bookshelf/156'],\n",
        "    ['Amphibians', 'https://www.gutenberg.org/ebooks/bookshelf/157'],\n",
        "    ['Trapping', 'https://www.gutenberg.org/ebooks/bookshelf/153'],\n",
        "    ['Wild', 'https://www.gutenberg.org/ebooks/bookshelf/152'],\n",
        "    ['Anthropology', 'https://www.gutenberg.org/ebooks/bookshelf/8'],\n",
        "    ['Archaeology', 'https://www.gutenberg.org/ebooks/bookshelf/9'],\n",
        "    ['Architecture', 'https://www.gutenberg.org/ebooks/bookshelf/10'],\n",
        "    ['Argentina', 'https://www.gutenberg.org/ebooks/bookshelf/112'],\n",
        "    ['Art', 'https://www.gutenberg.org/ebooks/bookshelf/11'],\n",
        "    ['Legends', 'https://www.gutenberg.org/ebooks/bookshelf/160'],\n",
        "    ['Astronomy', 'https://www.gutenberg.org/ebooks/bookshelf/101'],\n",
        "    ['Atheism', 'https://www.gutenberg.org/ebooks/bookshelf/199'],\n",
        "    ['Australia', 'https://www.gutenberg.org/ebooks/bookshelf/113'],\n",
        "    ['Racism', 'https://www.gutenberg.org/ebooks/bookshelf/65'],\n",
        "    ['Association', 'https://www.gutenberg.org/ebooks/bookshelf/422'],\n",
        "    ['America', 'https://www.gutenberg.org/ebooks/bookshelf/136'],\n",
        "    ['Listings', 'https://www.gutenberg.org/ebooks/bookshelf/13'],\n",
        "    ['Bibliomania', 'https://www.gutenberg.org/ebooks/bookshelf/15'],\n",
        "    ['Biographies', 'https://www.gutenberg.org/ebooks/bookshelf/16'],\n",
        "    ['Biology', 'https://www.gutenberg.org/ebooks/bookshelf/201'],\n",
        "    ['Botany', 'https://www.gutenberg.org/ebooks/bookshelf/115'],\n",
        "    ['War', 'https://www.gutenberg.org/ebooks/bookshelf/137'],\n",
        "    ['Law', 'https://www.gutenberg.org/ebooks/bookshelf/205'],\n",
        "    ['Buddhism', 'https://www.gutenberg.org/ebooks/bookshelf/116'],\n",
        "    ['Camping', 'https://www.gutenberg.org/ebooks/bookshelf/148'],\n",
        "    ['Canada', 'https://www.gutenberg.org/ebooks/bookshelf/118'],\n",
        "    ['Chemistry', 'https://www.gutenberg.org/ebooks/bookshelf/211'],\n",
        "    ['Series', 'https://www.gutenberg.org/ebooks/bookshelf/17'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/18'],\n",
        "    ['History', 'https://www.gutenberg.org/ebooks/bookshelf/19'],\n",
        "    ['Literature', 'https://www.gutenberg.org/ebooks/bookshelf/20'],\n",
        "    ['Books', 'https://www.gutenberg.org/ebooks/bookshelf/22'],\n",
        "    ['Christianity', 'https://www.gutenberg.org/ebooks/bookshelf/119'],\n",
        "    ['Christmas', 'https://www.gutenberg.org/ebooks/bookshelf/23'],\n",
        "    ['Antiquity', 'https://www.gutenberg.org/ebooks/bookshelf/24'],\n",
        "    ['Cooking', 'https://www.gutenberg.org/ebooks/bookshelf/419'],\n",
        "    ['Crafts', 'https://www.gutenberg.org/ebooks/bookshelf/27'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/28'],\n",
        "    ['Nonfiction', 'https://www.gutenberg.org/ebooks/bookshelf/29'],\n",
        "    ['History', 'https://www.gutenberg.org/ebooks/bookshelf/220'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/30'],\n",
        "    ['Society', 'https://www.gutenberg.org/ebooks/bookshelf/31'],\n",
        "    ['Engineering', 'https://www.gutenberg.org/ebooks/bookshelf/32'],\n",
        "    ['War', 'https://www.gutenberg.org/ebooks/bookshelf/139'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/33'],\n",
        "    ['Esperanto', 'https://www.gutenberg.org/ebooks/bookshelf/34'],\n",
        "    ['Ecology', 'https://www.gutenberg.org/ebooks/bookshelf/224'],\n",
        "    ['Education', 'https://www.gutenberg.org/ebooks/bookshelf/138'],\n",
        "    ['Egypt', 'https://www.gutenberg.org/ebooks/bookshelf/121'],\n",
        "    ['Fantasy', 'https://www.gutenberg.org/ebooks/bookshelf/36'],\n",
        "    ['Folklore', 'https://www.gutenberg.org/ebooks/bookshelf/37'],\n",
        "    ['Forestry', 'https://www.gutenberg.org/ebooks/bookshelf/145'],\n",
        "    ['France', 'https://www.gutenberg.org/ebooks/bookshelf/122'],\n",
        "    ['Forest', 'https://www.gutenberg.org/ebooks/bookshelf/226'],\n",
        "    ['Geology', 'https://www.gutenberg.org/ebooks/bookshelf/227'],\n",
        "    ['Germany', 'https://www.gutenberg.org/ebooks/bookshelf/123'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/39'],\n",
        "    ['Greece', 'https://www.gutenberg.org/ebooks/bookshelf/124'],\n",
        "    ['Classics', 'https://www.gutenberg.org/ebooks/bookshelf/40'],\n",
        "    ['Hinduism', 'https://www.gutenberg.org/ebooks/bookshelf/125'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/41'],\n",
        "    ['Horror', 'https://www.gutenberg.org/ebooks/bookshelf/42'],\n",
        "    ['Horticulture', 'https://www.gutenberg.org/ebooks/bookshelf/43'],\n",
        "    ['Humor', 'https://www.gutenberg.org/ebooks/bookshelf/44'],\n",
        "    ['India', 'https://www.gutenberg.org/ebooks/bookshelf/45'],\n",
        "    ['Islam', 'https://www.gutenberg.org/ebooks/bookshelf/126'],\n",
        "    ['Italy', 'https://www.gutenberg.org/ebooks/bookshelf/127'],\n",
        "    ['Judaism', 'https://www.gutenberg.org/ebooks/bookshelf/128'],\n",
        "    ['Education', 'https://www.gutenberg.org/ebooks/bookshelf/46'],\n",
        "    ['Love', 'https://www.gutenberg.org/ebooks/bookshelf/47'],\n",
        "    ['Manufacturing', 'https://www.gutenberg.org/ebooks/bookshelf/146'],\n",
        "    ['Mathematics', 'https://www.gutenberg.org/ebooks/bookshelf/102'],\n",
        "    ['Medicine', 'https://www.gutenberg.org/ebooks/bookshelf/48'],\n",
        "    ['Microbiology', 'https://www.gutenberg.org/ebooks/bookshelf/105'],\n",
        "    ['Microscopy', 'https://www.gutenberg.org/ebooks/bookshelf/109'],\n",
        "    ['Books', 'https://www.gutenberg.org/ebooks/bookshelf/49'],\n",
        "    ['Music', 'https://www.gutenberg.org/ebooks/bookshelf/50'],\n",
        "    ['Mycology', 'https://www.gutenberg.org/ebooks/bookshelf/129'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/51'],\n",
        "    ['Mythology', 'https://www.gutenberg.org/ebooks/bookshelf/52'],\n",
        "    ['Bookshelf', 'https://www.gutenberg.org/ebooks/bookshelf/149'],\n",
        "    ['America', 'https://www.gutenberg.org/ebooks/bookshelf/53'],\n",
        "    ['History', 'https://www.gutenberg.org/ebooks/bookshelf/54'],\n",
        "    ['Zealand', 'https://www.gutenberg.org/ebooks/bookshelf/130'],\n",
        "    ['Association', 'https://www.gutenberg.org/ebooks/bookshelf/244'],\n",
        "    ['Norway', 'https://www.gutenberg.org/ebooks/bookshelf/131'],\n",
        "    ['Plays', 'https://www.gutenberg.org/ebooks/bookshelf/55'],\n",
        "    ['Opera', 'https://www.gutenberg.org/ebooks/bookshelf/56'],\n",
        "    ['Paganism', 'https://www.gutenberg.org/ebooks/bookshelf/132'],\n",
        "    ['Philosophy', 'https://www.gutenberg.org/ebooks/bookshelf/57'],\n",
        "    ['Photography', 'https://www.gutenberg.org/ebooks/bookshelf/158'],\n",
        "    ['Physics', 'https://www.gutenberg.org/ebooks/bookshelf/103'],\n",
        "    ['Physiology', 'https://www.gutenberg.org/ebooks/bookshelf/110'],\n",
        "    ['Plays', 'https://www.gutenberg.org/ebooks/bookshelf/59'],\n",
        "    ['Poetry', 'https://www.gutenberg.org/ebooks/bookshelf/60'],\n",
        "    ['Politics', 'https://www.gutenberg.org/ebooks/bookshelf/61'],\n",
        "    ['Precursors', 'https://www.gutenberg.org/ebooks/bookshelf/62'],\n",
        "    ['Gutenberg', 'https://www.gutenberg.org/ebooks/bookshelf/63'],\n",
        "    ['Psychology', 'https://www.gutenberg.org/ebooks/bookshelf/64'],\n",
        "    ['Reference', 'https://www.gutenberg.org/ebooks/bookshelf/66'],\n",
        "    ['Stories', 'https://www.gutenberg.org/ebooks/bookshelf/67'],\n",
        "    ['Science', 'https://www.gutenberg.org/ebooks/bookshelf/106'],\n",
        "    ['Fiction', 'https://www.gutenberg.org/ebooks/bookshelf/68'],\n",
        "    ['Women', 'https://www.gutenberg.org/ebooks/bookshelf/403'],\n",
        "    ['Scouts', 'https://www.gutenberg.org/ebooks/bookshelf/144'],\n",
        "    ['Stories', 'https://www.gutenberg.org/ebooks/bookshelf/69'],\n",
        "    ['Slavery', 'https://www.gutenberg.org/ebooks/bookshelf/70'],\n",
        "    ['Sociology', 'https://www.gutenberg.org/ebooks/bookshelf/134'],\n",
        "    ['Africa', 'https://www.gutenberg.org/ebooks/bookshelf/135'],\n",
        "    ['America', 'https://www.gutenberg.org/ebooks/bookshelf/71'],\n",
        "    ['War', 'https://www.gutenberg.org/ebooks/bookshelf/140'],\n",
        "    ['Suffrage', 'https://www.gutenberg.org/ebooks/bookshelf/72'],\n",
        "    ['Technology', 'https://www.gutenberg.org/ebooks/bookshelf/143'],\n",
        "    ['Microbiology', 'https://www.gutenberg.org/ebooks/bookshelf/105'],\n",
        "    ['Journal', 'https://www.gutenberg.org/ebooks/bookshelf/423'],\n",
        "    ['Transportation', 'https://www.gutenberg.org/ebooks/bookshelf/74'],\n",
        "    ['Travel', 'https://www.gutenberg.org/ebooks/bookshelf/75'],\n",
        "    ['War', 'https://www.gutenberg.org/ebooks/bookshelf/141'],\n",
        "    ['Kingdom', 'https://www.gutenberg.org/ebooks/bookshelf/76'],\n",
        "    ['States', 'https://www.gutenberg.org/ebooks/bookshelf/136'],\n",
        "    ['Law', 'https://www.gutenberg.org/ebooks/bookshelf/302'],\n",
        "    ['Western', 'https://www.gutenberg.org/ebooks/bookshelf/77'],\n",
        "    ['Witchcraft', 'https://www.gutenberg.org/ebooks/bookshelf/78'],\n",
        "    ['Journals', 'https://www.gutenberg.org/ebooks/bookshelf/80'],\n",
        "    ['Woodwork', 'https://www.gutenberg.org/ebooks/bookshelf/147'],\n",
        "    ['War', 'https://www.gutenberg.org/ebooks/bookshelf/142'],\n",
        "    ['War', 'https://www.gutenberg.org/ebooks/bookshelf/325'],\n",
        "    ['Zoology', 'https://www.gutenberg.org/ebooks/bookshelf/303']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEvTS668pw8M"
      },
      "source": [
        "Parsing several (25 maximum) texts per theme of books for these themes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPjqTG0xp9_B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import undetected_chromedriver as uc\n",
        "\n",
        "# Initialize the driver\n",
        "driver = uc.Chrome(headless=True)\n",
        "\n",
        "def download_file(url, save_path):\n",
        "    \"\"\"Download a file from URL to specified path\"\"\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def process_book(book_url, theme_dir):\n",
        "    \"\"\"Process individual book page and download if conditions are met\"\"\"\n",
        "    try:\n",
        "        driver.get(book_url)\n",
        "        print(f\"{theme_dir}:{book_url}\")\n",
        "\n",
        "        # 1. Check language (wait for element to load)\n",
        "        try:\n",
        "            lang_xpath = \"//table[@class='bibrec']//tr[th='Language']/td\"\n",
        "            lang_element = WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_element_located((By.XPATH, lang_xpath))\n",
        "            )\n",
        "            language = lang_element.text.strip()\n",
        "            print(f\"Language {language}\")\n",
        "            if language not in [\"English\", \"Английский\"]:\n",
        "                return False\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "        # 2. Extract reading ease score\n",
        "        try:\n",
        "            # Use presence_of_all_elements_located to find every element that matches\n",
        "            note_xpath = \"//table[@class='bibrec']//tr[th='Note']/td\"\n",
        "            note_elements = WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_all_elements_located((By.XPATH, note_xpath))\n",
        "            )\n",
        "\n",
        "            # Check if any notes were found\n",
        "            if not note_elements:\n",
        "                return False\n",
        "\n",
        "            all_notes_data = []\n",
        "            reading_score = None\n",
        "\n",
        "            # Loop through each found note element\n",
        "            for note_element in note_elements:\n",
        "                note_text = note_element.text.strip()\n",
        "                print(f\"Note: {note_text}\")\n",
        "\n",
        "                # Extract reading score from each note using regex\n",
        "                match = re.search(r\"Reading ease score:\\s*([\\d.]+)\", note_text)\n",
        "\n",
        "                current_score = match.group(1) if match else None\n",
        "\n",
        "                # Store the first valid reading score found\n",
        "                if current_score and reading_score is None:\n",
        "                    reading_score = current_score\n",
        "\n",
        "                all_notes_data.append({\n",
        "                    \"note_text\": note_text,\n",
        "                    \"reading_score\": current_score\n",
        "                })\n",
        "\n",
        "            # Check if at least one note had a reading score.\n",
        "            if reading_score is None:\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while parsing notes: {e}\")\n",
        "            return False\n",
        "\n",
        "        # 3. Find and process download link\n",
        "        try:\n",
        "            selectors = [\n",
        "                \"//a[@class='link ' and text()='Plain Text UTF-8']\"\n",
        "            ]\n",
        "\n",
        "            download_link = None\n",
        "            for selector in selectors:\n",
        "                try:\n",
        "                    # Use find_elements to avoid an exception if not found\n",
        "                    links = driver.find_elements(By.XPATH, selector)\n",
        "                    if links:\n",
        "                        download_link = links[0]\n",
        "                        break\n",
        "                except Exception:\n",
        "                    # Continue to the next selector\n",
        "                    continue\n",
        "\n",
        "            if not download_link:\n",
        "                print(\"Error: Download link not found.\")\n",
        "                return False\n",
        "\n",
        "            download_url = download_link.get_attribute('href')\n",
        "            book_id_match = re.search(r'/ebooks/(\\d+)', book_url)\n",
        "            if not book_id_match:\n",
        "                print(\"Error: Could not extract book ID from URL.\")\n",
        "                return False\n",
        "\n",
        "            book_id = book_id_match.group(1)\n",
        "            filename = f\"{book_id}_{reading_score}.txt\"\n",
        "            save_path = os.path.join(theme_dir, filename)\n",
        "\n",
        "            # Download the file\n",
        "            if download_file(download_url, save_path):\n",
        "                print(f\"Downloaded: {filename}\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"Failed to download: {filename}\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during download link processing: {e}\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {book_url}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "    return False\n",
        "\n",
        "# Main processing loop\n",
        "for theme_name, theme_url in processed_list:\n",
        "    try:\n",
        "        # Create theme directory\n",
        "        theme_dir = re.sub(r'[\\\\/*?:\"<>|]', \"\", theme_name)  # Sanitize directory name\n",
        "        os.makedirs('data/' + theme_dir, exist_ok=True)\n",
        "        print(f\"\\nProcessing theme: {theme_name}\")\n",
        "\n",
        "        # Navigate to bookshelf page\n",
        "        driver.get(theme_url)\n",
        "        time.sleep(2)  # Initial load wait\n",
        "\n",
        "        # Process pagination\n",
        "        page_count = 0\n",
        "        books_processed = 0\n",
        "        while True:\n",
        "            page_count += 1\n",
        "            print(f\"  Page {page_count}\")\n",
        "\n",
        "            # Find all book links\n",
        "            book_links = []\n",
        "            try:\n",
        "                book_elements = WebDriverWait(driver, 15).until(\n",
        "                    EC.presence_of_all_elements_located((By.XPATH, \"//li[@class='booklink']/a[@class='link']\"))\n",
        "                )\n",
        "                book_links = [elem.get_attribute('href') for elem in book_elements]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Process each book\n",
        "            for book_url in book_links:\n",
        "                if process_book(book_url, theme_dir):\n",
        "                    books_processed += 1\n",
        "                time.sleep(1)  # Be polite to server\n",
        "\n",
        "            # Check for next page\n",
        "            try:\n",
        "                next_button = driver.find_element(By.XPATH, \"//a[@title='Go to next page']\")\n",
        "                if \"disabled\" in next_button.get_attribute(\"class\"):\n",
        "                    break\n",
        "\n",
        "                next_button.click()\n",
        "                time.sleep(3)  # Wait for page load\n",
        "            except:\n",
        "                break\n",
        "\n",
        "        print(f\"  Finished theme: {theme_name} | Books downloaded: {books_processed}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing theme {theme_name}: {str(e)}\")\n",
        "\n",
        "# Clean up\n",
        "driver.quit()\n",
        "print(\"\\nProcessing completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRDi8IjqwhP1"
      },
      "source": [
        "After 3 hours of executing, we get 1.4 Gb of text data with theme and text reading complexity separation. File saved in directories by theme, file name saved as `{BookId}_{TextComplexity}.txt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL8U2cd-4Bm9",
        "outputId": "2c2c3d9b-0e05-44a2-9625-885bb47f9911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adventure, Africa, America, Amphibians, Anarchism, Animal, Anthropology, Antiquity, Archaeology, Architecture, Argentina, Art, Association, Astronomy, Atheism, Australia, Bibliomania, Biographies, Biology, Birds, Books, Bookshelf, Botany, Buddhism, Camping, Canada, Chemistry, Christianity, Christmas, Classics, Cooking, Crafts, Domestic, Ecology, Education, Egypt, Engineering, Esperanto, Fantasy, Fiction, Folklore, Forest, Forestry, France, Geology, Germany, Greece, Gutenberg, Hinduism, History, Horror, Horticulture, Humor, India, Insects, Islam, Italy, Journal, Journals, Judaism, Kingdom, Law, Legends, Listings, Literature, Love, Mammals, Manufacturing, Mathematics, Medicine, Microbiology, Microscopy, Music, Mycology, Mythology, Nonfiction, Norway, Opera, Paganism, Philosophy, Photography, Physics, Physiology, Plays, Poetry, Politics, Precursors, Psychology, Racism, Reference, Science, Scouts, Series, Slavery, Society, Sociology, States, Stories, Suffrage, Technology, Transportation, Trapping, Travel, War, Western, Wild, Witchcraft, Women, Woodwork, Zealand, Zoology\n"
          ]
        }
      ],
      "source": [
        "dirs = [element[0] for element in processed_list]\n",
        "dirs = list(set(dirs))\n",
        "dirs.sort()\n",
        "print(\", \".join(dirs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg4eR5046bFk"
      },
      "source": [
        "Now creating \"one word\" dataset with these columns:\n",
        "\n",
        "| Word | Adventure | Africa | ... | Zealand | Zoology | Total | Complexity |\n",
        "|-|-|-|-|-|-|-|-|\n",
        "| `str` | `int` | `int` | ... | `int` | `int` | `int` | `float` |\n",
        "\n",
        "by this algorithm:\n",
        "1. Open text from every directory (theme)\n",
        "2. Clear and tokenize text by NLTK, deleting stop words  \n",
        "2. Adding every token:\n",
        "  - if exist, update theme counter, total counter and complexity by this formula:\n",
        "  $\n",
        "  \\text{complexity}_{\\text{new}} = \\frac{\\text{comlexity}_\\text{old}\\cdot \\text{total}+\\text{text_complexity}}{\\text{total}+1}\n",
        "  $\n",
        "  - if not exit, add word update counters and set complxity to text complexity of current text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFEVgONg92C2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\79150\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\79150\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\79150\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\79150\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 111 themes: Adventure, Africa, America, Amphibians, Anarchism, Animal, Anthropology, Antiquity, Archaeology, Architecture, Argentina, Art, Association, Astronomy, Atheism, Australia, Bibliomania, Biographies, Biology, Birds, Books, Bookshelf, Botany, Buddhism, Camping, Canada, Chemistry, Christianity, Christmas, Classics, Cooking, Crafts, Domestic, Ecology, Education, Egypt, Engineering, Esperanto, Fantasy, Fiction, Folklore, Forest, Forestry, France, Geology, Germany, Greece, Gutenberg, Hinduism, History, Horror, Horticulture, Humor, India, Insects, Islam, Italy, Journal, Journals, Judaism, Kingdom, Law, Legends, Listings, Literature, Love, Mammals, Manufacturing, Mathematics, Medicine, Microbiology, Microscopy, Music, Mycology, Mythology, Nonfiction, Norway, Opera, Paganism, Philosophy, Photography, Physics, Physiology, Plays, Poetry, Politics, Precursors, Psychology, Racism, Reference, Science, Scouts, Series, Slavery, Society, Sociology, States, Stories, Suffrage, Technology, Transportation, Trapping, Travel, War, Western, Wild, Witchcraft, Women, Woodwork, Zealand, Zoology\n",
            "Processing theme: Adventure\n",
            "Processing theme: Africa\n",
            "Processing theme: America\n",
            "Processing theme: Amphibians\n",
            "Processing theme: Anarchism\n",
            "Processing theme: Animal\n",
            "Processing theme: Anthropology\n",
            "Processing theme: Antiquity\n",
            "Processing theme: Archaeology\n",
            "Processing theme: Architecture\n",
            "Processing theme: Argentina\n",
            "Processing theme: Art\n",
            "Processing theme: Association\n",
            "Processing theme: Astronomy\n",
            "Processing theme: Atheism\n",
            "Processing theme: Australia\n",
            "Processing theme: Bibliomania\n",
            "Processing theme: Biographies\n",
            "Processing theme: Biology\n",
            "Processing theme: Birds\n",
            "Processing theme: Books\n",
            "Processing theme: Bookshelf\n",
            "Processing theme: Botany\n",
            "Processing theme: Buddhism\n",
            "Processing theme: Camping\n",
            "Processing theme: Canada\n",
            "Processing theme: Chemistry\n",
            "Processing theme: Christianity\n",
            "Processing theme: Christmas\n",
            "Processing theme: Classics\n",
            "Processing theme: Cooking\n",
            "Processing theme: Crafts\n",
            "Processing theme: Domestic\n",
            "Processing theme: Ecology\n",
            "Processing theme: Education\n",
            "Processing theme: Egypt\n",
            "Processing theme: Engineering\n",
            "Processing theme: Esperanto\n",
            "Processing theme: Fantasy\n",
            "Processing theme: Fiction\n",
            "Processing theme: Folklore\n",
            "Processing theme: Forest\n",
            "Processing theme: Forestry\n",
            "Processing theme: France\n",
            "Processing theme: Geology\n",
            "Processing theme: Germany\n",
            "Processing theme: Greece\n",
            "Processing theme: Gutenberg\n",
            "Processing theme: Hinduism\n",
            "Processing theme: History\n",
            "Processing theme: Horror\n",
            "Processing theme: Horticulture\n",
            "Processing theme: Humor\n",
            "Processing theme: India\n",
            "Processing theme: Insects\n",
            "Processing theme: Islam\n",
            "Processing theme: Italy\n",
            "Processing theme: Journal\n",
            "Processing theme: Journals\n",
            "Processing theme: Judaism\n",
            "Processing theme: Kingdom\n",
            "Processing theme: Law\n",
            "Processing theme: Legends\n",
            "Processing theme: Listings\n",
            "Processing theme: Literature\n",
            "Processing theme: Love\n",
            "Processing theme: Mammals\n",
            "Processing theme: Manufacturing\n",
            "Processing theme: Mathematics\n",
            "Processing theme: Medicine\n",
            "Processing theme: Microbiology\n",
            "Processing theme: Microscopy\n",
            "Processing theme: Music\n",
            "Processing theme: Mycology\n",
            "Processing theme: Mythology\n",
            "Processing theme: Nonfiction\n",
            "Processing theme: Norway\n",
            "Processing theme: Opera\n",
            "Processing theme: Paganism\n",
            "Processing theme: Philosophy\n",
            "Processing theme: Photography\n",
            "Processing theme: Physics\n",
            "Processing theme: Physiology\n",
            "Processing theme: Plays\n",
            "Processing theme: Poetry\n",
            "Processing theme: Politics\n",
            "Processing theme: Precursors\n",
            "Processing theme: Psychology\n",
            "Processing theme: Racism\n",
            "Processing theme: Reference\n",
            "Processing theme: Science\n",
            "Processing theme: Scouts\n",
            "Processing theme: Series\n",
            "Processing theme: Slavery\n",
            "Processing theme: Society\n",
            "Processing theme: Sociology\n",
            "Processing theme: States\n",
            "Processing theme: Stories\n",
            "Processing theme: Suffrage\n",
            "Processing theme: Technology\n",
            "Processing theme: Transportation\n",
            "Processing theme: Trapping\n",
            "Processing theme: Travel\n",
            "Processing theme: War\n",
            "Processing theme: Western\n",
            "Processing theme: Wild\n",
            "Processing theme: Witchcraft\n",
            "Processing theme: Women\n",
            "Processing theme: Woodwork\n",
            "Processing theme: Zealand\n",
            "Processing theme: Zoology\n",
            "\n",
            "Dataset created with 1351701 unique words\n",
            "Top 10 most frequent words:\n",
            "       Word   Total  Complexity\n",
            "52      one  749024   67.143501\n",
            "608   would  477428   67.192777\n",
            "140    said  460270   74.127681\n",
            "683    time  406210   66.731788\n",
            "17      may  384135   64.051919\n",
            "1232   work  351692   66.587452\n",
            "54      man  326321   68.929421\n",
            "945    upon  322783   65.671136\n",
            "161     two  321086   66.728969\n",
            "6       day  301614   68.404165\n",
            "\n",
            "Dataset saved to word_dataset.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Tokenize, clean, and lemmatize text\"\"\"\n",
        "    # Convert to lowercase and remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and short tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "def process_directory(base_dir, dirs):\n",
        "    \"\"\"Process all theme directories and create the word dataset\"\"\"\n",
        "    # Filter themes to only include those in dirs list\n",
        "    themes = [d for d in dirs if os.path.isdir(os.path.join(base_dir, d))]\n",
        "    print(f\"Processing {len(themes)} themes: {', '.join(themes)}\")\n",
        "\n",
        "    # Initialize word dictionary\n",
        "    word_data = {}\n",
        "\n",
        "    # Process each theme\n",
        "    for theme in themes:\n",
        "        theme_dir = os.path.join(base_dir, theme)\n",
        "        print(f\"Processing theme: {theme}\")\n",
        "\n",
        "        # Process each file in the theme directory\n",
        "        for filename in os.listdir(theme_dir):\n",
        "            if filename.endswith('.txt'):\n",
        "                # Extract reading score from filename\n",
        "                try:\n",
        "                    book_id, reading_score = filename.split('_')[:2]\n",
        "                    reading_score = float(reading_score.replace('.txt', ''))\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing filename {filename}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                filepath = os.path.join(theme_dir, filename)\n",
        "\n",
        "                try:\n",
        "                    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        text = f.read()\n",
        "\n",
        "                    # Preprocess text\n",
        "                    tokens = preprocess_text(text)\n",
        "\n",
        "                    # Update word data\n",
        "                    for word in tokens:\n",
        "                        if word not in word_data:\n",
        "                            # Initialize new word entry\n",
        "                            word_data[word] = {\n",
        "                                'Total': 0,\n",
        "                                'Complexity': 0.0,\n",
        "                                **{t: 0 for t in themes}  # Initialize all themes to 0\n",
        "                            }\n",
        "\n",
        "                        # Get current word stats\n",
        "                        current = word_data[word]\n",
        "                        total_old = current['Total']\n",
        "\n",
        "                        # Update counts\n",
        "                        current['Total'] += 1\n",
        "                        current[theme] += 1\n",
        "\n",
        "                        # Update complexity using moving average formula\n",
        "                        current['Complexity'] = (\n",
        "                            (current['Complexity'] * total_old) + reading_score\n",
        "                        ) / (total_old + 1)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame.from_dict(word_data, orient='index')\n",
        "    df.reset_index(inplace=True)\n",
        "    df.rename(columns={'index': 'Word'}, inplace=True)\n",
        "\n",
        "    # Reorder columns: Word, themes, Total, Complexity\n",
        "    columns = ['Word'] + themes + ['Total', 'Complexity']\n",
        "    return df[columns]\n",
        "\n",
        "# Main processing\n",
        "base_directory = \"Data\"  # Directory containing theme subdirectories\n",
        "output_file = \"word_dataset.csv\"\n",
        "\n",
        "# Use the dirs array from the previous cell\n",
        "df = process_directory(base_directory, dirs)\n",
        "\n",
        "# Display some statistics\n",
        "print(f\"\\nDataset created with {len(df)} unique words\")\n",
        "print(f\"Top 10 most frequent words:\")\n",
        "print(df.sort_values('Total', ascending=False).head(10)[['Word', 'Total', 'Complexity']])\n",
        "\n",
        "# Save results\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"\\nDataset saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Total</th>\n",
              "      <th>Complexity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>one</td>\n",
              "      <td>749024</td>\n",
              "      <td>67.143501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>608</th>\n",
              "      <td>would</td>\n",
              "      <td>477428</td>\n",
              "      <td>67.192777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>said</td>\n",
              "      <td>460270</td>\n",
              "      <td>74.127681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>683</th>\n",
              "      <td>time</td>\n",
              "      <td>406210</td>\n",
              "      <td>66.731788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>may</td>\n",
              "      <td>384135</td>\n",
              "      <td>64.051919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1437</th>\n",
              "      <td>set</td>\n",
              "      <td>113125</td>\n",
              "      <td>67.948997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>order</td>\n",
              "      <td>111802</td>\n",
              "      <td>63.708995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>869</th>\n",
              "      <td>far</td>\n",
              "      <td>111664</td>\n",
              "      <td>65.489456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>world</td>\n",
              "      <td>111031</td>\n",
              "      <td>65.826793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>whole</td>\n",
              "      <td>110181</td>\n",
              "      <td>64.430272</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Word   Total  Complexity\n",
              "52      one  749024   67.143501\n",
              "608   would  477428   67.192777\n",
              "140    said  460270   74.127681\n",
              "683    time  406210   66.731788\n",
              "17      may  384135   64.051919\n",
              "...     ...     ...         ...\n",
              "1437    set  113125   67.948997\n",
              "229   order  111802   63.708995\n",
              "869     far  111664   65.489456\n",
              "4     world  111031   65.826793\n",
              "122   whole  110181   64.430272\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('word_dataset.csv')\n",
        "df = df[df['Total'] > 10]\n",
        "\n",
        "df.sort_values('Total', ascending=False).head(100)[['Word', 'Total', 'Complexity']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Oxford dictionary parsing\n",
        "The obtained results reflect the theoretical correspondence between the thematicity of words and their complexity. However, for frequent common words that occur very often, their complexity as well as their thematic belonging become close to the average values. The same happens with words belonging to several subthemes. In addition to the above, we have also collected proper names, Roman numerals and other words that are of no value to an English language learner.\n",
        "\n",
        "In order to create a list of really useful words for learning, it was decided to sparse word lists from an authoritative source. This source was the oxford dictionary. It has a division by topical, subtopic and subsubtopic, as well as its CEFR difficulty and part of speech. After research and testing, the following parser was written:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.wait import WebDriverWait\n",
        "import undetected_chromedriver as uc\n",
        "\n",
        "\n",
        "# Initialize the driver\n",
        "driver = uc.Chrome()\n",
        "driver.maximize_window()\n",
        "\n",
        "# Create an empty DataFrame\n",
        "columns = ['word', 'topic', 'subtopic', 'subsubtopic', 'CEFR_level', 'word_class', 'link']\n",
        "df = pd.DataFrame(columns=columns)\n",
        "\n",
        "try:\n",
        "    # Step 1: Parse all topics from the main page\n",
        "    main_url = \"https://www.oxfordlearnersdictionaries.com/topic/\"\n",
        "    driver.get(main_url)\n",
        "    time.sleep(0.5)  # Allow page to load\n",
        "\n",
        "    topic_elements = driver.find_elements(By.CLASS_NAME, 'topic-box')\n",
        "        # By.CSS_SELECTOR, 'div.topic-content-container > div > a')\n",
        "    print(f\"Found {len(topic_elements)} topic elements\")\n",
        "\n",
        "    topics_data = []\n",
        "    for topic_elem in topic_elements:\n",
        "        topic_name = topic_elem.find_element(By.CLASS_NAME, 'topic-label').text.strip()\n",
        "        topic_elem = topic_elem.find_element(By.TAG_NAME, 'a')\n",
        "        topic_link = topic_elem.get_attribute('href')\n",
        "        topics_data.append((topic_name, topic_link))\n",
        "\n",
        "    # Step 2: Process each topic\n",
        "    for topic_name, topic_link in topics_data:\n",
        "        print(f\"Processing topic: {topic_name} - {topic_link}\")\n",
        "        driver.get(topic_link)\n",
        "        time.sleep(0.3)  # Allow page to load\n",
        "\n",
        "        # Find all subtopic boxes\n",
        "        subtopic_boxes = driver.find_elements(By.CLASS_NAME, 'topic-box-secondary-heading')\n",
        "        print(f\"  Found {len(subtopic_boxes)} subtopic boxes:\")\n",
        "\n",
        "        subtopic_boxes_links = []\n",
        "        subtopic_names = []\n",
        "        for box in subtopic_boxes:\n",
        "            # Extract subtopic name\n",
        "            subtopic_name = box.text.strip()\n",
        "            if '(see all)' in subtopic_name:\n",
        "                subtopic_name = subtopic_name.replace('(see all)', '')\n",
        "            print(f'text:{subtopic_name}')\n",
        "            subtopic_names.append(subtopic_name)\n",
        "\n",
        "            # Find the word list within the box\n",
        "\n",
        "            link = box.get_attribute('href')\n",
        "            subtopic_boxes_links.append(link)\n",
        "\n",
        "        for i in range(len(subtopic_boxes_links)):\n",
        "            link = subtopic_boxes_links[i]\n",
        "            subtopic_name = subtopic_names[i]\n",
        "            try:\n",
        "                driver.get(link)\n",
        "                # word_list = box.find_element(By.CLASS_NAME, 'top-g')\n",
        "                # time.sleep(100)\n",
        "\n",
        "                wait = WebDriverWait(driver, 10)\n",
        "                word_list = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'top-g')))\n",
        "\n",
        "                word_items = word_list.find_elements(By.TAG_NAME, 'li')\n",
        "\n",
        "                print(f\"    Found {len(word_items)} words in subtopic: {subtopic_name}\")\n",
        "\n",
        "                for word_li in word_items:\n",
        "                    try:\n",
        "                        # Extract word and word class\n",
        "                        word = word_li.find_element(By.TAG_NAME, 'a').text\n",
        "                        word_class = word_li.find_element(By.CLASS_NAME, 'pos').text\n",
        "                        link = word_li.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
        "\n",
        "                        # Extract CEFR level and subsubtopic from data attributes\n",
        "                        cefr_level = None\n",
        "                        subsubtopic = None\n",
        "                        attrs = driver.execute_script(\n",
        "                            'var items = {}; for (index = 0; index < arguments[0].attributes.length; ++index) { items[arguments[0].attributes[index].name] = arguments[0].attributes[index].value }; return items;',\n",
        "                            word_li\n",
        "                        )\n",
        "                        for attr, value in attrs.items():\n",
        "                            if attr.endswith('_t'):\n",
        "                                cefr_level = value\n",
        "                                subsubtopic = attr[:-2]  # Remove '_t' suffix\n",
        "                                subsubtopic = subsubtopic[5:] # remove \"data-\"\n",
        "                                break\n",
        "\n",
        "                        # Append to DataFrame\n",
        "                        new_row = {\n",
        "                            'word': word,\n",
        "                            'topic': topic_name,\n",
        "                            'subtopic': subtopic_name,\n",
        "                            'subsubtopic': subsubtopic,\n",
        "                            'CEFR_level': cefr_level,\n",
        "                            'word_class': word_class,\n",
        "                            'link': link\n",
        "                        }\n",
        "                        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"      Error processing a word: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"    Error processing a subtopic box: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "finally:\n",
        "    driver.quit()\n",
        "    # Save to CSV\n",
        "    df.to_csv('oxford_vocabulary.csv', index=False)\n",
        "    print(\"Data saved to 'oxford_vocabulary.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the parsing was done on a small number of pages, the data was collected in just a few hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Translation of words and examples\n",
        "From the key date, we obtained an authoritative list of words, their level and part of speech, and investigated the frequency of word usage in English literature. But since our app is built for learning English, we need translations of these words, as well as example sentences using them both for generating word help and for generating the tasks themselves, where the word under study is dropped from the sentence and its context is used to create 3 other options for the user to choose from (see NLP part from Anton).\n",
        "\n",
        "The following websites were analyzed:  \n",
        "\n",
        "| Website | Functionality | Translation | Transcription | Example Sentences with Translation | Collocations | CAPTCHA | Notes |  \n",
        "|---------|--------------|-------------|---------------|------------------------------------|--------------|---------|-------|  \n",
        "| [Yandex Translate](https://translate.yandex.ru/dictionary/Английский-Русский/fluently) | Translator | ✅ | ✅ | ✅ | ❌ | ✅ | Split by meanings |  \n",
        "| [WooordHunt](https://wooordhunt.ru/word/fluently) | Dictionary | ✅ | ✅ | ⚠️ (merged, not split by meanings) | ✅ | ❌ | All translations combined |  \n",
        "| [KartaSlov](https://en.kartaslov.ru/перевод-в-контексте/fluently) | Contextual Dictionary | ✅ | ✅ | ✅ | ❌ | ❌ | Focus on context |  \n",
        "| [Cambridge Dictionary](https://dictionary.cambridge.org/ru/словарь/английский/fluently) | English Dictionary | ❌ | ✅ | ⚠️ (no translation) | ❌ | ❌ | Authentic examples |  \n",
        "\n",
        "\n",
        "At first I tried to create parser for yandex translate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.wait import WebDriverWait\n",
        "import undetected_chromedriver as uc\n",
        "\n",
        "# Initialize the driver\n",
        "driver = uc.Chrome()\n",
        "driver.maximize_window()\n",
        "\n",
        "df = pd.read_csv(\"oxford_vocabulary.csv\")\n",
        "\n",
        "def translate(word):\n",
        "    try:\n",
        "        driver.get(f'https://translate.yandex.ru/dictionary/Английский-Русский/{word}')\n",
        "        result = \"\"\n",
        "        not_first_time = False\n",
        "        while result == \"\":\n",
        "            if not_first_time:\n",
        "                 time.sleep(0.1)\n",
        "            wait = WebDriverWait(driver, 10)\n",
        "            box = wait.until(EC.text_to_be_present_in_element((By.ID, 'fakeArea'), word))\n",
        "            wait = WebDriverWait(driver, 10)\n",
        "            box = wait.until(EC.presence_of_element_located((By.ID, 'dstBox')))\n",
        "            area = box.find_element(By.TAG_NAME, 'p')\n",
        "            texts = area.find_elements(By.TAG_NAME, 'span')\n",
        "            result = \"\".join([text.text for text in texts])\n",
        "            not_first_time = True\n",
        "        return result\n",
        "    except:\n",
        "        translate(word)\n",
        "\n",
        "print(translate(\"something\"))\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "but at the third request captcha occures. So I decided to parse KartaSlov service:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.wait import WebDriverWait\n",
        "import undetected_chromedriver as uc\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "def clean_word(word):\n",
        "    \"\"\"Cleans the word of unwanted characters and replaces spaces with %20\"\"\"\n",
        "    word = re.sub(r'[^\\w\\s\\-]', '', word)  # Deleting non-letter characters (except spaces and hyphens)\n",
        "    word = re.sub(r'\\s+', ' ', word).strip()  # Removing unnecessary spaces\n",
        "    return \"%20\".join(word.split())  # Replacing spaces with %20\n",
        "def translate(word, attemps = 0, max_attempts = 1, fast_translation = False):\n",
        "    word = \"%20\".join(word.split())\n",
        "    try:\n",
        "        driver.get(f'https://en.kartaslov.ru/перевод-в-контексте/{word}')\n",
        "        results = []\n",
        "        translations = []\n",
        "        sentences = [[]]\n",
        "        not_first_time = False\n",
        "        while translations == []:\n",
        "            # If page is not completely loaded, sleep a bit and continue\n",
        "            if not_first_time:\n",
        "                time.sleep(0.1)\n",
        "\n",
        "            # Page loading and fast translations\n",
        "            wait = WebDriverWait(driver, 10)\n",
        "            fast_headers = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'v2-ctx-tra-list')))\n",
        "            fast_translation_boxes = fast_headers.find_elements(By.TAG_NAME, 'li')\n",
        "            print(f\"Fast translations:{', '.join([box.text for box in fast_translation_boxes])}\")\n",
        "            if fast_translation:\n",
        "                translations = [box.text for box in fast_translation_boxes]\n",
        "                for translation in translations:\n",
        "                    result = {\n",
        "                        'translation': translation,\n",
        "                        'sentences' : sentences\n",
        "                    }\n",
        "                    results.append(result)\n",
        "                return results\n",
        "\n",
        "            # Words translations by headers in boxes\n",
        "            headers = driver.find_elements(By.CLASS_NAME, 'v2-tr-header')\n",
        "            translations = []\n",
        "            for header in headers:\n",
        "                translation_box = header.find_element(By.TAG_NAME, 'a')\n",
        "                translation = translation_box.text\n",
        "                translations.append(translation)\n",
        "\n",
        "            # Sentences words and translations\n",
        "            sentences = driver.find_elements(By.CLASS_NAME,'v2-tr-row')\n",
        "            sentences_list = []\n",
        "            print(f\"Всего {len(sentences)}, по {len(sentences)/len(translations)} на слово\")\n",
        "            for i in range(len(sentences)):\n",
        "                sentence = sentences[i]\n",
        "                sentence_en = sentence.find_element(By.CSS_SELECTOR,\".v2-tr-column-src.v2-tr-text\")\n",
        "                sentence_en = sentence_en.text\n",
        "\n",
        "                sentence_ru = sentence.find_element(By.CSS_SELECTOR,\".v2-tr-column-trg.v2-tr-text\")\n",
        "                sentence_ru = sentence_ru.text\n",
        "                sentences_list.append([sentence_en, sentence_ru])\n",
        "            if len(sentences_list)>=5:\n",
        "                for i in range(0,len(sentences_list)-5,5):\n",
        "                    result = {\n",
        "                        'translation': translations[i//5],\n",
        "                        'sentences' : sentences_list[i:(i+5)]\n",
        "                    }\n",
        "                    results.append(result)\n",
        "            else:\n",
        "                results = [{\n",
        "                        'translation': translations[0],\n",
        "                        'sentences' : sentences_list\n",
        "                }]\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        attemps += 1\n",
        "        if attemps == max_attempts:\n",
        "            return None\n",
        "        else:\n",
        "            print(f\"Error {e} \\nAttempt #{attemps} for word '{word}'\")\n",
        "            translate(word, attemps)\n",
        "\n",
        "driver = uc.Chrome()\n",
        "driver.set_page_load_timeout(0.1)\n",
        "\n",
        "df = pd.read_csv(\"oxford_vocabulary.csv\")\n",
        "output_file = \"translated_vocabulary.csv\"\n",
        "\n",
        "# Loading existing results, if the file exists\n",
        "if os.path.exists(output_file):\n",
        "    result_df = pd.read_csv(output_file)\n",
        "    \n",
        "    processed_words = set(result_df['word_clean'].dropna().unique())\n",
        "    last_word = result_df.tail(1)['word'].tolist()[0]\n",
        "    start_index = df[df['word'] == last_word].index[0] + 1\n",
        "else:\n",
        "    start_index = 0\n",
        "    result_df = pd.DataFrame(columns=[\"word\", \"word_clean\", \"translation\", \"sentences\", \"original_index\"])\n",
        "\n",
        "for index, row in tqdm(df.iloc[start_index:].iterrows(), total=len(df)-start_index, initial=start_index):\n",
        "    original_word = row[\"word\"]\n",
        "    \n",
        "    if not isinstance(original_word, str) or original_word.strip() == \"\":\n",
        "        continue\n",
        "        \n",
        "    cleaned = clean_word(original_word)\n",
        "    \n",
        "    if cleaned in processed_words:\n",
        "        continue\n",
        "        \n",
        "    translations = translate(cleaned)\n",
        "    \n",
        "    if translations is None:\n",
        "        print(f\"Не удалось получить перевод для: {original_word}\")\n",
        "        continue\n",
        "        \n",
        "    for trans in translations:\n",
        "        new_row = {\n",
        "            \"word\": original_word,\n",
        "            \"word_clean\": cleaned,\n",
        "            \"translation\": trans[\"translation\"],\n",
        "            \"sentences\": trans[\"sentences\"],\n",
        "            \"original_index\": index\n",
        "        }\n",
        "        result_df = pd.concat([result_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "    \n",
        "    # Regular saving of results\n",
        "    if index % 100 == 0:\n",
        "        result_df.to_csv(output_file, index=False)\n",
        "    \n",
        "    # Updating the list of processed words\n",
        "    processed_words.add(cleaned)\n",
        "\n",
        "result_df.to_csv(output_file, index=False)\n",
        "driver.quit()\n",
        "print(\"Processing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After several hours and 7050/29997 words translated, my IP was banned (only for my pc service was down):\n",
        "![503 error](image.png)\n",
        "So I rewrite script to start from where it crashed and run on other team member's PC. After approximatelly 14 hours in summary, script completely parsed all words. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final results\n",
        "We ended up with an Oxford dictionary without translations, translations for words from this dictionary with example sentences with translations, and we also have a list of all words from literature with their popularity. We remove the unnecessary, add key columns for each of the tables and for the resulting table we do a cleanup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total</th>\n",
              "      <th>word</th>\n",
              "      <th>topic</th>\n",
              "      <th>subtopic</th>\n",
              "      <th>subsubtopic</th>\n",
              "      <th>CEFR_level</th>\n",
              "      <th>translation</th>\n",
              "      <th>sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>462.0</td>\n",
              "      <td>adder</td>\n",
              "      <td>Animals</td>\n",
              "      <td>Animals</td>\n",
              "      <td>amphibians_and_reptiles</td>\n",
              "      <td>c2</td>\n",
              "      <td>гадюка</td>\n",
              "      <td>[['No poison darts tipped with the venom of an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>770.0</td>\n",
              "      <td>alligator</td>\n",
              "      <td>Animals</td>\n",
              "      <td>Animals</td>\n",
              "      <td>amphibians_and_reptiles</td>\n",
              "      <td>c1</td>\n",
              "      <td>аллигатор</td>\n",
              "      <td>[[\"Tell him I've been bit by an alligator.\", '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>770.0</td>\n",
              "      <td>alligator</td>\n",
              "      <td>Animals</td>\n",
              "      <td>Animals</td>\n",
              "      <td>amphibians_and_reptiles</td>\n",
              "      <td>c1</td>\n",
              "      <td>крокодил</td>\n",
              "      <td>[['You wanted to honor the man by showing him ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>770.0</td>\n",
              "      <td>alligator</td>\n",
              "      <td>Animals</td>\n",
              "      <td>Animals</td>\n",
              "      <td>amphibians_and_reptiles</td>\n",
              "      <td>c1</td>\n",
              "      <td>из крокодиловой кожи</td>\n",
              "      <td>[['Even that alligator handbag his wife left o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>196.0</td>\n",
              "      <td>alpaca</td>\n",
              "      <td>Animals</td>\n",
              "      <td>Animals</td>\n",
              "      <td>farm_animals</td>\n",
              "      <td>c2</td>\n",
              "      <td>альпака</td>\n",
              "      <td>[[\"— It's very lightweight alpaca.\", 'Альпака?...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34738</th>\n",
              "      <td>4.0</td>\n",
              "      <td>workplace</td>\n",
              "      <td>Work and business</td>\n",
              "      <td>Working life</td>\n",
              "      <td>office_life</td>\n",
              "      <td>b2</td>\n",
              "      <td>на работе</td>\n",
              "      <td>[['These laws were designed to protect a secre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34739</th>\n",
              "      <td>4.0</td>\n",
              "      <td>workplace</td>\n",
              "      <td>Work and business</td>\n",
              "      <td>Working life</td>\n",
              "      <td>office_life</td>\n",
              "      <td>b2</td>\n",
              "      <td>рабочий</td>\n",
              "      <td>[['Devon, looking at my computer is a violatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34740</th>\n",
              "      <td>4.0</td>\n",
              "      <td>workplace</td>\n",
              "      <td>Work and business</td>\n",
              "      <td>Working life</td>\n",
              "      <td>office_life</td>\n",
              "      <td>b2</td>\n",
              "      <td>работать</td>\n",
              "      <td>[['Oh, my God, this is a workplace.', 'Господи...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34741</th>\n",
              "      <td>4.0</td>\n",
              "      <td>workplace</td>\n",
              "      <td>Work and business</td>\n",
              "      <td>Working life</td>\n",
              "      <td>office_life</td>\n",
              "      <td>b2</td>\n",
              "      <td>служебный</td>\n",
              "      <td>[['The old workplace romance trick.', 'Старый ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34742</th>\n",
              "      <td>4.0</td>\n",
              "      <td>workplace</td>\n",
              "      <td>Work and business</td>\n",
              "      <td>Working life</td>\n",
              "      <td>office_life</td>\n",
              "      <td>b2</td>\n",
              "      <td>офис</td>\n",
              "      <td>[['Got any workplace survival tips for me?', '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>34743 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Total       word              topic      subtopic  \\\n",
              "0      462.0      adder            Animals       Animals   \n",
              "1      770.0  alligator            Animals       Animals   \n",
              "2      770.0  alligator            Animals       Animals   \n",
              "3      770.0  alligator            Animals       Animals   \n",
              "4      196.0     alpaca            Animals       Animals   \n",
              "...      ...        ...                ...           ...   \n",
              "34738    4.0  workplace  Work and business  Working life   \n",
              "34739    4.0  workplace  Work and business  Working life   \n",
              "34740    4.0  workplace  Work and business  Working life   \n",
              "34741    4.0  workplace  Work and business  Working life   \n",
              "34742    4.0  workplace  Work and business  Working life   \n",
              "\n",
              "                   subsubtopic CEFR_level           translation  \\\n",
              "0      amphibians_and_reptiles         c2                гадюка   \n",
              "1      amphibians_and_reptiles         c1             аллигатор   \n",
              "2      amphibians_and_reptiles         c1              крокодил   \n",
              "3      amphibians_and_reptiles         c1  из крокодиловой кожи   \n",
              "4                 farm_animals         c2               альпака   \n",
              "...                        ...        ...                   ...   \n",
              "34738              office_life         b2             на работе   \n",
              "34739              office_life         b2               рабочий   \n",
              "34740              office_life         b2              работать   \n",
              "34741              office_life         b2             служебный   \n",
              "34742              office_life         b2                  офис   \n",
              "\n",
              "                                               sentences  \n",
              "0      [['No poison darts tipped with the venom of an...  \n",
              "1      [[\"Tell him I've been bit by an alligator.\", '...  \n",
              "2      [['You wanted to honor the man by showing him ...  \n",
              "3      [['Even that alligator handbag his wife left o...  \n",
              "4      [[\"— It's very lightweight alpaca.\", 'Альпака?...  \n",
              "...                                                  ...  \n",
              "34738  [['These laws were designed to protect a secre...  \n",
              "34739  [['Devon, looking at my computer is a violatio...  \n",
              "34740  [['Oh, my God, this is a workplace.', 'Господи...  \n",
              "34741  [['The old workplace romance trick.', 'Старый ...  \n",
              "34742  [['Got any workplace survival tips for me?', '...  \n",
              "\n",
              "[34743 rows x 8 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "translated = pd.read_csv('translated_vocabulary.csv')\n",
        "translated.drop(columns = ['word_clean', 'original_index'], inplace = True)\n",
        "\n",
        "oxford = pd.read_csv('oxford_vocabulary.csv')\n",
        "oxford.drop(columns = ['word_class', 'link'], inplace = True)\n",
        "\n",
        "result = pd.merge(oxford,translated, on = 'word', how = 'right')\n",
        "result.drop_duplicates(inplace=True)\n",
        "\n",
        "clean = result[~result['sentences'].duplicated(keep=False)]\n",
        "\n",
        "words = pd.read_csv('word_dataset.csv')\n",
        "result = pd.merge(words[['Word', 'Total']], clean, right_on = 'word', left_on = 'Word', how = 'right')\n",
        "\n",
        "result.drop(columns = ['Word'], inplace = True)\n",
        "result.to_csv('data.csv')\n",
        "\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
